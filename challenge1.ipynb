{
 "cells": [
  {
   "cell_type": "code",
   "id": "6088e7ec-a663-4f7a-ad27-b8fdfa395e39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:05:47.889370Z",
     "start_time": "2026-02-08T21:05:47.547273Z"
    }
   },
   "source": [
    "pip install huggingface_hub"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "96dd3e42-47a6-4c82-9deb-25bcd1489640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:05:47.920163Z",
     "start_time": "2026-02-08T21:05:47.889965Z"
    }
   },
   "source": [
    "%reset -f"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d1afc494-8f85-4242-b525-5f584340f5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:05:49.850968Z",
     "start_time": "2026-02-08T21:05:47.920589Z"
    }
   },
   "source": [
    "# Step 0: Install All Necessary Libraries\n",
    "# -------------------------------------------------------------------\n",
    "print(\"‚öôÔ∏è Installing required libraries...\")\n",
    "print(\"‚úÖ Libraries are ready.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Step 1: Load All Data from the Official Server Path\n",
    "# -------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"üìÇ Loading all source data files from the server...\")\n",
    "REPO_ID = \"lainmn/AgentDS-Healthcare\"\n",
    "admissions_train = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/admissions_train.csv\", repo_type=\"dataset\"))\n",
    "patients = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/patients.csv\", repo_type=\"dataset\"))\n",
    "ed_cost_train = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/ed_cost_train.csv\", repo_type=\"dataset\"))\n",
    "admissions_test = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/admissions_test.csv\", repo_type=\"dataset\"))\n",
    "ed_cost_test = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/ed_cost_test.csv\", repo_type=\"dataset\"))\n",
    "with open(hf_hub_download(REPO_ID, \"Healthcare/discharge_notes.json\", repo_type=\"dataset\"), 'r') as f:\n",
    "    discharge_notes = json.load(f)\n",
    "notes_df = pd.DataFrame(discharge_notes)\n",
    "print(\"‚úÖ All data files loaded successfully.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Step 2: Define Feature Engineering and Prepare Full Datasets\n",
    "# -------------------------------------------------------------------\n",
    "def feature_engineer(df):\n",
    "    \"\"\"Applies our tabular feature engineering steps.\"\"\"\n",
    "    categorical_cols = ['sex', 'insurance', 'primary_dx']\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, dummy_na=False, dtype=int)\n",
    "    df['is_weekend_discharge'] = df['discharge_weekday'].isin([6, 7]).astype(int)\n",
    "    df['avg_cost_per_ed_visit'] = df['prior_ed_cost_5y_usd'] / df['prior_ed_visits_5y']\n",
    "    df['avg_cost_per_ed_visit'] = df['avg_cost_per_ed_visit'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    return df\n",
    "\n",
    "print(\"üß† Preparing the full training dataset (Tabular + NLP)...\")\n",
    "# Prepare Training Data\n",
    "train_df = pd.merge(admissions_train, patients, on='patient_id', how='left')\n",
    "train_df = pd.merge(train_df, ed_cost_train, on='patient_id', how='left')\n",
    "train_df_with_notes = pd.merge(train_df, notes_df, on='admission_id', how='left')\n",
    "train_df_with_notes['note'] = train_df_with_notes['note'].fillna('')\n",
    "train_df_featured = feature_engineer(train_df_with_notes.copy())\n",
    "\n",
    "# NLP Feature Engineering (TF-IDF)\n",
    "tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "\n",
    "# Corrected method from .fit_ to .fit_transform\n",
    "X_notes_train = tfidf.fit_transform(train_df_featured['note'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X_notes_train.toarray(), columns=['note_' + col for col in tfidf.get_feature_names_out()])\n",
    "final_train_df = pd.concat([train_df_featured.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Define final X_full and y_full for training\n",
    "y_full = final_train_df['readmit_30d']\n",
    "X_full = final_train_df.drop(columns=['admission_id', 'patient_id', 'readmit_30d', 'note', 'ed_cost_next3y_usd', 'zip3', 'primary_chronic'], errors='ignore')\n",
    "X_full = X_full.loc[:, ~X_full.columns.duplicated()].fillna(0)\n",
    "print(\"‚úÖ Full training dataset ready.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"üìù Preparing the official TEST dataset...\")\n",
    "# Prepare Test Data\n",
    "test_df = pd.merge(admissions_test, patients, on='patient_id', how='left')\n",
    "test_df = pd.merge(test_df, ed_cost_test, on='patient_id', how='left')\n",
    "test_df_with_notes = pd.merge(test_df, notes_df, on='admission_id', how='left')\n",
    "test_df_with_notes['note'] = test_df_with_notes['note'].fillna('')\n",
    "final_test_df = feature_engineer(test_df_with_notes.copy())\n",
    "X_notes_test = tfidf.transform(final_test_df['note'])\n",
    "tfidf_test_df = pd.DataFrame(X_notes_test.toarray(), columns=['note_' + col for col in tfidf.get_feature_names_out()])\n",
    "final_test_features = pd.concat([final_test_df.reset_index(drop=True), tfidf_test_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Align columns perfectly and save IDs\n",
    "test_ids_final = final_test_features['admission_id']\n",
    "final_test_aligned = final_test_features.reindex(columns=X_full.columns, fill_value=0)\n",
    "print(\"‚úÖ Test dataset is ready and aligned.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Step 3: Train Final Model, Predict, and Submit\n",
    "# -------------------------------------------------------------------\n",
    "print(\"ü§ñ Training the final, optimized XGBoost model on ALL data...\")\n",
    "# Use the best parameters we found with Optuna\n",
    "best_params = {'n_estimators': 957, 'learning_rate': 0.2793, 'max_depth': 8, 'subsample': 0.792, 'colsample_bytree': 0.643, 'gamma': 2.891}\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'logloss'\n",
    "best_params['random_state'] = 42\n",
    "best_params['scale_pos_weight'] = (y_full == 0).sum() / (y_full == 1).sum()\n",
    "\n",
    "optimized_model = xgb.XGBClassifier(**best_params)\n",
    "optimized_model.fit(X_full, y_full)\n",
    "print(\"‚úÖ Final XGBoost model is trained.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"üöÄ Generating final predictions...\")\n",
    "final_predictions = optimized_model.predict(final_test_aligned.fillna(0))\n",
    "submission_df = pd.DataFrame({'admission_id': test_ids_final, 'readmit_30d': final_predictions})\n",
    "submission_df.to_csv(\"XGB_ONLY_predictions.csv\", index=False)\n",
    "print(\"‚úÖ Submission file 'XGB_ONLY_predictions.csv' created.\")\n",
    "\n",
    ""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final XGBoost model is trained.\n",
      "--------------------------------------------------\n",
      "üöÄ Generating final predictions...\n",
      "‚úÖ Submission file 'XGB_ONLY_predictions.csv' created.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "d58872f7-0b44-46aa-8dc8-1452206a921e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:05:51.498783Z",
     "start_time": "2026-02-08T21:05:49.880965Z"
    }
   },
   "source": "# ============================================================\n# Score Estimation via Stratified K-Fold Cross-Validation\n# (Test labels are not available on HuggingFace, so we estimate\n#  the Macro-F1 score using cross-validation on training data)\n# ============================================================\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import f1_score, classification_report\n\nprint(\"üìä Estimating Macro-F1 score via 5-fold Stratified CV on training data...\")\n\nbest_params_cv = {\n    'n_estimators': 957,\n    'learning_rate': 0.2793,\n    'max_depth': 8,\n    'subsample': 0.792,\n    'colsample_bytree': 0.643,\n    'gamma': 2.891,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss',\n    'random_state': 42,\n    'scale_pos_weight': (y_full == 0).sum() / (y_full == 1).sum()\n}\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_predictions = cross_val_predict(\n    xgb.XGBClassifier(**best_params_cv),\n    X_full, y_full, cv=cv, n_jobs=-1\n)\n\nmacro_f1 = f1_score(y_full, cv_predictions, average='macro')\nprint(f\"\\n{'='*50}\")\nprint(f\"  Estimated Macro-F1 Score: {macro_f1:.4f}\")\nprint(f\"{'='*50}\")\nprint(f\"\\n{classification_report(y_full, cv_predictions, target_names=['No Readmit', 'Readmit'])}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  Estimated Macro-F1 Score: 0.8456\n",
      "==================================================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Readmit       0.85      0.84      0.84      2479\n",
      "     Readmit       0.84      0.85      0.85      2521\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d9c1e744-47de-498d-a4a9-a396e171e1df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:12:00.353410Z",
     "start_time": "2026-02-08T21:05:51.526024Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Improved Stacking - Simple & Robust Approach to Beat 0.8960\n",
    "\n",
    "Focus on what works:\n",
    "1. More diverse XGBoost configurations\n",
    "2. Enhanced features (proven ones)\n",
    "3. Better meta-learner with probability stacking\n",
    "4. No complex feature selection (simpler = more robust)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from huggingface_hub import hf_hub_download\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def improved_feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Feature engineering with proven high-value features.\n",
    "    \"\"\"\n",
    "    # One-hot encoding\n",
    "    categorical_cols = ['sex', 'insurance', 'primary_dx']\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, dummy_na=False, dtype=int)\n",
    "    \n",
    "    # Core temporal features\n",
    "    df['is_weekend_discharge'] = df['discharge_weekday'].isin([6, 7]).astype(int)\n",
    "    df['is_monday'] = (df['discharge_weekday'] == 1).astype(int)\n",
    "    df['is_friday'] = (df['discharge_weekday'] == 5).astype(int)\n",
    "    \n",
    "    # Cost features\n",
    "    df['avg_cost_per_ed_visit'] = df['prior_ed_cost_5y_usd'] / (df['prior_ed_visits_5y'] + 1)\n",
    "    df['avg_cost_per_ed_visit'] = df['avg_cost_per_ed_visit'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    # Age features\n",
    "    df['is_elderly'] = (df['age'] >= 65).astype(int)\n",
    "    df['is_very_elderly'] = (df['age'] >= 80).astype(int)\n",
    "    df['age_squared'] = df['age'] ** 2\n",
    "    \n",
    "    # Charlson features\n",
    "    df['high_charlson'] = (df['charlson_band'] >= 3).astype(int)\n",
    "    df['charlson_squared'] = df['charlson_band'] ** 2\n",
    "    \n",
    "    # ED visit features\n",
    "    df['recent_ed_ratio'] = df['ed_visits_6m'] / (df['prior_ed_visits_5y'] + 1)\n",
    "    df['is_frequent_ed'] = (df['ed_visits_6m'] >= 2).astype(int)\n",
    "    df['is_very_frequent_ed'] = (df['ed_visits_6m'] >= 3).astype(int)\n",
    "    df['is_new_patient'] = (df['prior_ed_visits_5y'] == 0).astype(int)\n",
    "    \n",
    "    # LOS features\n",
    "    df['los_log'] = np.log1p(df['los_days'])\n",
    "    df['is_long_stay'] = (df['los_days'] >= 5).astype(int)\n",
    "    \n",
    "    # Key interactions\n",
    "    df['age_x_charlson'] = df['age'] * df['charlson_band']\n",
    "    df['age_x_ed'] = df['age'] * df['ed_visits_6m']\n",
    "    df['charlson_x_ed'] = df['charlson_band'] * df['ed_visits_6m']\n",
    "    df['los_x_ed'] = df['los_days'] * df['ed_visits_6m']\n",
    "    \n",
    "    # Risk scores\n",
    "    df['risk_score_1'] = (\n",
    "        df['age'] / 100 + \n",
    "        df['charlson_band'] * 0.5 + \n",
    "        df['ed_visits_6m'] * 0.3\n",
    "    )\n",
    "    \n",
    "    df['risk_score_2'] = (\n",
    "        df['is_elderly'] * 2 +\n",
    "        df['high_charlson'] * 3 +\n",
    "        df['is_frequent_ed'] * 2\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Improved stacking to beat 0.8960.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"IMPROVED STACKING - Target: >0.90 F1\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Check GPU\n",
    "    build_info = xgb.build_info()\n",
    "    gpu_available = build_info.get('USE_CUDA', False)\n",
    "    print(f\"\\n{'‚úÖ GPU Enabled' if gpu_available else '‚ö†Ô∏è  CPU Mode'}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nüìÇ Loading data...\")\n",
    "    REPO_ID = \"lainmn/AgentDS-Healthcare\"\n",
    "    \n",
    "    admissions_train = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/admissions_train.csv\", repo_type=\"dataset\"))\n",
    "    patients = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/patients.csv\", repo_type=\"dataset\"))\n",
    "    ed_cost_train = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/ed_cost_train.csv\", repo_type=\"dataset\"))\n",
    "    admissions_test = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/admissions_test.csv\", repo_type=\"dataset\"))\n",
    "    ed_cost_test = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/ed_cost_test.csv\", repo_type=\"dataset\"))\n",
    "    \n",
    "    with open(hf_hub_download(REPO_ID, \"Healthcare/discharge_notes.json\", repo_type=\"dataset\"), 'r') as f:\n",
    "        discharge_notes = json.load(f)\n",
    "    notes_df = pd.DataFrame(discharge_notes)\n",
    "    \n",
    "    # Prepare training data\n",
    "    print(\"\\nüîß Feature engineering...\")\n",
    "    train_df = pd.merge(admissions_train, patients, on='patient_id', how='left')\n",
    "    train_df = pd.merge(train_df, ed_cost_train, on='patient_id', how='left')\n",
    "    train_df = pd.merge(train_df, notes_df, on='admission_id', how='left')\n",
    "    train_df['note'] = train_df['note'].fillna('')\n",
    "    \n",
    "    train_df_featured = improved_feature_engineering(train_df.copy())\n",
    "    \n",
    "    # TF-IDF - increase to 850 features with trigrams\n",
    "    print(\"   Creating enhanced TF-IDF (850 features with trigrams)...\")\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=850,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 3),  # Trigrams for better context\n",
    "        min_df=3,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    \n",
    "    X_notes_train = tfidf.fit_transform(train_df_featured['note'])\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        X_notes_train.toarray(),\n",
    "        columns=['note_' + col for col in tfidf.get_feature_names_out()]\n",
    "    )\n",
    "    \n",
    "    final_train_df = pd.concat([\n",
    "        train_df_featured.reset_index(drop=True),\n",
    "        tfidf_df.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    y_full = final_train_df['readmit_30d']\n",
    "    X_full = final_train_df.drop(columns=[\n",
    "        'admission_id', 'patient_id', 'readmit_30d', 'note',\n",
    "        'ed_cost_next3y_usd', 'zip3', 'primary_chronic'\n",
    "    ], errors='ignore')\n",
    "    \n",
    "    X_full = X_full.loc[:, ~X_full.columns.duplicated()].fillna(0)\n",
    "    print(f\"‚úÖ Features ready: {X_full.shape}\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    print(\"\\nüìù Preparing test data...\")\n",
    "    test_df = pd.merge(admissions_test, patients, on='patient_id', how='left')\n",
    "    test_df = pd.merge(test_df, ed_cost_test, on='patient_id', how='left')\n",
    "    test_df = pd.merge(test_df, notes_df, on='admission_id', how='left')\n",
    "    test_df['note'] = test_df['note'].fillna('')\n",
    "    \n",
    "    final_test_df = improved_feature_engineering(test_df.copy())\n",
    "    \n",
    "    X_text_test = tfidf.transform(final_test_df['note'])\n",
    "    tfidf_test_df = pd.DataFrame(\n",
    "        X_text_test.toarray(),\n",
    "        columns=['note_' + col for col in tfidf.get_feature_names_out()]\n",
    "    )\n",
    "    \n",
    "    final_test_features = pd.concat([\n",
    "        final_test_df.reset_index(drop=True),\n",
    "        tfidf_test_df.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    test_ids = final_test_features['admission_id']\n",
    "    final_test_aligned = final_test_features.reindex(columns=X_full.columns, fill_value=0).fillna(0)\n",
    "    print(f\"‚úÖ Test data ready: {final_test_aligned.shape}\")\n",
    "    \n",
    "    # Build improved stacking ensemble\n",
    "    print(\"\\nü§ñ Building Improved Stacking Ensemble...\")\n",
    "    print(\"   5 diverse base models with 8-fold CV\")\n",
    "    \n",
    "    scale_pos_weight = (y_full == 0).sum() / (y_full == 1).sum()\n",
    "    \n",
    "    # Base Model 1: Your optimized XGBoost\n",
    "    xgb_opt = Pipeline([\n",
    "        ('model', xgb.XGBClassifier(\n",
    "            n_estimators=771,\n",
    "            learning_rate=0.03002156989594884,\n",
    "            max_depth=7,\n",
    "            subsample=0.6844646638528962,\n",
    "            colsample_bytree=0.8063408933100336,\n",
    "            gamma=3.418014628166754,\n",
    "            objective='binary:logistic',\n",
    "            random_state=42,\n",
    "            device='cuda' if gpu_available else 'cpu',\n",
    "            tree_method='hist',\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Base Model 2: XGBoost - more trees, lower learning rate\n",
    "    xgb_deep = Pipeline([\n",
    "        ('model', xgb.XGBClassifier(\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.015,\n",
    "            max_depth=6,\n",
    "            subsample=0.75,\n",
    "            colsample_bytree=0.85,\n",
    "            gamma=2.0,\n",
    "            min_child_weight=2,\n",
    "            objective='binary:logistic',\n",
    "            random_state=999,\n",
    "            device='cuda' if gpu_available else 'cpu',\n",
    "            tree_method='hist',\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Base Model 3: XGBoost - shallow and wide\n",
    "    xgb_shallow = Pipeline([\n",
    "        ('model', xgb.XGBClassifier(\n",
    "            n_estimators=1200,\n",
    "            learning_rate=0.02,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.9,\n",
    "            gamma=1.0,\n",
    "            objective='binary:logistic',\n",
    "            random_state=555,\n",
    "            device='cuda' if gpu_available else 'cpu',\n",
    "            tree_method='hist',\n",
    "            scale_pos_weight=scale_pos_weight\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Base Model 4: Logistic Regression with L2\n",
    "    lr_l2 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(\n",
    "            C=0.5,\n",
    "            class_weight='balanced',\n",
    "            solver='saga',\n",
    "            penalty='l2',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Base Model 5: Logistic Regression with L1 (different features selected)\n",
    "    lr_l1 = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(\n",
    "            C=0.3,\n",
    "            class_weight='balanced',\n",
    "            solver='saga',\n",
    "            penalty='l1',\n",
    "            max_iter=1000,\n",
    "            random_state=777\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Define base models\n",
    "    estimators = [\n",
    "        ('xgb_opt', xgb_opt),\n",
    "        ('xgb_deep', xgb_deep),\n",
    "        ('xgb_shallow', xgb_shallow),\n",
    "        ('lr_l2', lr_l2),\n",
    "        ('lr_l1', lr_l1)\n",
    "    ]\n",
    "    \n",
    "    # Meta-learner: Logistic Regression\n",
    "    meta_learner = LogisticRegression(\n",
    "        C=1.5,\n",
    "        class_weight='balanced',\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create stacking classifier with 8-fold CV\n",
    "    stacker = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=StratifiedKFold(n_splits=8, shuffle=True, random_state=42),\n",
    "        stack_method='predict_proba',\n",
    "        passthrough=False,\n",
    "        n_jobs=1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Ensemble configured: 5 models √ó 8 folds = 40 base models + meta-learner\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nüöÄ Training Improved Stacking Ensemble...\")\n",
    "    print(\"   Expected time: 15-25 minutes\")\n",
    "    print()\n",
    "    \n",
    "    stacker.fit(X_full, y_full)\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"\\nüì§ Generating predictions...\")\n",
    "    final_predictions = stacker.predict(final_test_aligned)\n",
    "    \n",
    "    # Get probabilities\n",
    "    final_probas = stacker.predict_proba(final_test_aligned)[:, 1]\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'admission_id': test_ids,\n",
    "        'readmit_30d': final_predictions\n",
    "    })\n",
    "    \n",
    "    submission_filename = 'readmission_predictions_improved.csv'\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    \n",
    "    # Save probabilities\n",
    "    proba_df = pd.DataFrame({\n",
    "        'admission_id': test_ids,\n",
    "        'probability': final_probas\n",
    "    })\n",
    "    proba_df.to_csv('readmission_probabilities.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úÖ IMPROVED STACKING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Submission file: '{submission_filename}'\")\n",
    "    print(f\"Probabilities: 'readmission_probabilities.csv'\")\n",
    "    print(f\"Total predictions: {len(final_predictions)}\")\n",
    "    print(f\"Predicted readmissions: {final_predictions.sum()} ({100*final_predictions.mean():.2f}%)\")\n",
    "    print(f\"\\nYour baseline: 0.8960\")\n",
    "    print(f\"Expected: 0.900-0.910 F1\")\n",
    "    print(\"\\nüí° Key Improvements:\")\n",
    "    print(\"   ‚Ä¢ 850 TF-IDF features (vs 750) with trigrams\")\n",
    "    print(\"   ‚Ä¢ 5 diverse base models (3 XGBoost + 2 LR)\")\n",
    "    print(\"   ‚Ä¢ 8-fold CV for stable meta-model\")\n",
    "    print(\"   ‚Ä¢ Enhanced features (age¬≤, charlson¬≤, risk scores)\")\n",
    "    print(\"   ‚Ä¢ L1 + L2 regularized LR for feature diversity\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  2.8min finished\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "cac4863a-b2ae-4e32-8394-afaddb592e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:47:48.531732Z",
     "start_time": "2026-02-08T21:22:40.203425Z"
    }
   },
   "source": "# ============================================================\n# Score Estimation for Improved Stacking Ensemble\n# via Stratified K-Fold Cross-Validation\n# ============================================================\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import Pipeline\n\nprint(\"üìä Estimating Macro-F1 for Improved Stacking Ensemble via 5-fold CV...\")\nprint(\"   (This will take several minutes as it trains the full ensemble per fold)\\n\")\n\n# Re-use X_full, y_full from the stacking cell's main() scope\n# Re-run data prep inline to ensure variables are available\nREPO_ID = \"lainmn/AgentDS-Healthcare\"\nadmissions_train = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/admissions_train.csv\", repo_type=\"dataset\"))\npatients = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/patients.csv\", repo_type=\"dataset\"))\ned_cost_train = pd.read_csv(hf_hub_download(REPO_ID, \"Healthcare/ed_cost_train.csv\", repo_type=\"dataset\"))\n\nwith open(hf_hub_download(REPO_ID, \"Healthcare/discharge_notes.json\", repo_type=\"dataset\"), 'r') as f:\n    discharge_notes_cv = json.load(f)\nnotes_df_cv = pd.DataFrame(discharge_notes_cv)\n\ntrain_df_cv = pd.merge(admissions_train, patients, on='patient_id', how='left')\ntrain_df_cv = pd.merge(train_df_cv, ed_cost_train, on='patient_id', how='left')\ntrain_df_cv = pd.merge(train_df_cv, notes_df_cv, on='admission_id', how='left')\ntrain_df_cv['note'] = train_df_cv['note'].fillna('')\n\ntrain_df_cv_feat = improved_feature_engineering(train_df_cv.copy())\n\ntfidf_cv = TfidfVectorizer(\n    max_features=850, stop_words='english', ngram_range=(1, 3),\n    min_df=3, max_df=0.95, sublinear_tf=True\n)\nX_notes_cv = tfidf_cv.fit_transform(train_df_cv_feat['note'])\ntfidf_cv_df = pd.DataFrame(X_notes_cv.toarray(), columns=['note_' + c for c in tfidf_cv.get_feature_names_out()])\nfinal_cv_df = pd.concat([train_df_cv_feat.reset_index(drop=True), tfidf_cv_df.reset_index(drop=True)], axis=1)\n\ny_cv = final_cv_df['readmit_30d']\nX_cv = final_cv_df.drop(columns=['admission_id', 'patient_id', 'readmit_30d', 'note',\n                                   'ed_cost_next3y_usd', 'zip3', 'primary_chronic'], errors='ignore')\nX_cv = X_cv.loc[:, ~X_cv.columns.duplicated()].fillna(0)\n\nscale_pos_weight_cv = (y_cv == 0).sum() / (y_cv == 1).sum()\nbuild_info = xgb.build_info()\ngpu_available = build_info.get('USE_CUDA', False)\ndevice = 'cuda' if gpu_available else 'cpu'\n\n# Build the same stacking ensemble\nestimators_cv = [\n    ('xgb_opt', Pipeline([('model', xgb.XGBClassifier(\n        n_estimators=771, learning_rate=0.03, max_depth=7, subsample=0.684,\n        colsample_bytree=0.806, gamma=3.418, objective='binary:logistic',\n        random_state=42, device=device, tree_method='hist', scale_pos_weight=scale_pos_weight_cv))])),\n    ('xgb_deep', Pipeline([('model', xgb.XGBClassifier(\n        n_estimators=1500, learning_rate=0.015, max_depth=6, subsample=0.75,\n        colsample_bytree=0.85, gamma=2.0, min_child_weight=2, objective='binary:logistic',\n        random_state=999, device=device, tree_method='hist', scale_pos_weight=scale_pos_weight_cv))])),\n    ('xgb_shallow', Pipeline([('model', xgb.XGBClassifier(\n        n_estimators=1200, learning_rate=0.02, max_depth=5, subsample=0.8,\n        colsample_bytree=0.9, gamma=1.0, objective='binary:logistic',\n        random_state=555, device=device, tree_method='hist', scale_pos_weight=scale_pos_weight_cv))])),\n    ('lr_l2', Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(\n        C=0.5, class_weight='balanced', solver='saga', penalty='l2', max_iter=1000, random_state=42))])),\n    ('lr_l1', Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(\n        C=0.3, class_weight='balanced', solver='saga', penalty='l1', max_iter=1000, random_state=777))])),\n]\n\nstacker_cv = StackingClassifier(\n    estimators=estimators_cv,\n    final_estimator=LogisticRegression(C=1.5, class_weight='balanced', solver='lbfgs', max_iter=1000, random_state=42),\n    cv=StratifiedKFold(n_splits=8, shuffle=True, random_state=42),\n    stack_method='predict_proba', passthrough=False, n_jobs=1\n)\n\n# Outer 5-fold CV to estimate generalization score\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_preds_stacking = cross_val_predict(stacker_cv, X_cv, y_cv, cv=outer_cv, n_jobs=1)\n\nmacro_f1_stacking = f1_score(y_cv, cv_preds_stacking, average='macro')\nprint(f\"\\n{'='*60}\")\nprint(f\"  Simple XGBoost Macro-F1 (from earlier):  0.8456\")\nprint(f\"  Stacking Ensemble Macro-F1 (5-fold CV):  {macro_f1_stacking:.4f}\")\nprint(f\"{'='*60}\")\nprint(f\"\\n{classification_report(y_cv, cv_preds_stacking, target_names=['No Readmit', 'Readmit'])}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Simple XGBoost Macro-F1 (from earlier):  0.8456\n",
      "  Stacking Ensemble Macro-F1 (5-fold CV):  0.8956\n",
      "============================================================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Readmit       0.90      0.89      0.89      2479\n",
      "     Readmit       0.90      0.90      0.90      2521\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "B691-Pranathi",
   "language": "python",
   "name": "lapula-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
