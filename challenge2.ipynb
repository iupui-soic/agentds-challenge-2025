{
 "cells": [
  {
   "cell_type": "code",
   "id": "fb30fec7-1924-4a4a-a296-2ba44c952b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:01.905654Z",
     "start_time": "2026-02-08T21:55:01.535687Z"
    }
   },
   "source": [
    "pip install huggingface_hub PyPDF2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "09868494-46d2-496f-b74b-a56d3a84e30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:02.745021Z",
     "start_time": "2026-02-08T21:55:01.906378Z"
    }
   },
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport re\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\n\n# PDF extraction\nimport PyPDF2\n\n# HuggingFace data loading\nimport os\nfrom huggingface_hub import hf_hub_download, snapshot_download",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ccd296ab-f1c5-4125-a308-5b1c231ec785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:03.803659Z",
     "start_time": "2026-02-08T21:55:02.761082Z"
    }
   },
   "source": [
    "REPO_ID = \"lainmn/AgentDS-Healthcare\"\n",
    "\n",
    "# Download CSV files from HuggingFace\n",
    "TRAIN_CSV = hf_hub_download(REPO_ID, \"Healthcare/ed_cost_train.csv\", repo_type=\"dataset\")\n",
    "TEST_CSV = hf_hub_download(REPO_ID, \"Healthcare/ed_cost_test.csv\", repo_type=\"dataset\")\n",
    "\n",
    "# Download PDF receipts folder\n",
    "dataset_path = snapshot_download(REPO_ID, repo_type=\"dataset\", allow_patterns=\"Healthcare/receipts_pdf/*\")\n",
    "PDF_FOLDER = os.path.join(dataset_path, \"Healthcare\", \"receipts_pdf\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "725cb0c3-9e1e-42cb-8ab2-fe0a71537ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:03.818776Z",
     "start_time": "2026-02-08T21:55:03.809754Z"
    }
   },
   "source": [
    "def extract_pdf_features(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "        \n",
    "        features = {}\n",
    "        patient_match = re.search(r'Patient ID:\\s*(\\d+)', text)\n",
    "        if not patient_match:\n",
    "            return None\n",
    "        features['patient_id'] = int(patient_match.group(1))\n",
    "        \n",
    "        zip_match = re.search(r'ZIP3:\\s*(\\d+)', text)\n",
    "        features['zip3'] = int(zip_match.group(1)) if zip_match else 0\n",
    "        \n",
    "        ins_match = re.search(r'Insurance:\\s*(\\w+)', text)\n",
    "        features['insurance'] = ins_match.group(1).lower() if ins_match else 'unknown'\n",
    "        \n",
    "        total_match = re.search(r'TOTAL\\s+([\\d,]+\\.?\\d*)', text)\n",
    "        features['pdf_total_cost'] = float(total_match.group(1).replace(',', '')) if total_match else 0\n",
    "        \n",
    "        cpt_pattern = r'(\\d{5})\\s+(.+?)\\s+(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)'\n",
    "        cpt_matches = re.findall(cpt_pattern, text)\n",
    "        features['pdf_num_line_items'] = len(cpt_matches)\n",
    "        \n",
    "        if cpt_matches:\n",
    "            cpt_codes = [match[0] for match in cpt_matches]\n",
    "            ed_visit_codes = [c for c in cpt_codes if c.startswith('9928')]\n",
    "            features['pdf_num_ed_visits'] = len(ed_visit_codes)\n",
    "            features['pdf_high_complexity_visits'] = sum(1 for c in ed_visit_codes if c in ['99284', '99285'])\n",
    "            features['pdf_num_lab_tests'] = sum(1 for c in cpt_codes if 80000 <= int(c) <= 89999)\n",
    "            features['pdf_num_imaging'] = sum(1 for c in cpt_codes if 70000 <= int(c) <= 79999)\n",
    "            \n",
    "            line_costs = [float(match[4]) for match in cpt_matches]\n",
    "            features['pdf_avg_line_cost'] = np.mean(line_costs)\n",
    "            features['pdf_max_line_cost'] = np.max(line_costs)\n",
    "            features['pdf_std_line_cost'] = np.std(line_costs)\n",
    "            features['pdf_avg_cost_per_visit'] = features['pdf_total_cost'] / features['pdf_num_ed_visits'] if features['pdf_num_ed_visits'] > 0 else 0\n",
    "        else:\n",
    "            for key in ['pdf_num_ed_visits', 'pdf_high_complexity_visits', 'pdf_num_lab_tests', \n",
    "                       'pdf_num_imaging', 'pdf_avg_line_cost', 'pdf_max_line_cost', \n",
    "                       'pdf_std_line_cost', 'pdf_avg_cost_per_visit']:\n",
    "                features[key] = 0\n",
    "        \n",
    "        return features\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def load_pdf_features(pdf_folder):\n",
    "    pdf_folder = Path(pdf_folder)\n",
    "    pdf_files = list(pdf_folder.glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_features = []\n",
    "    for pdf_file in pdf_files:\n",
    "        features = extract_pdf_features(pdf_file)\n",
    "        if features:\n",
    "            all_features.append(features)\n",
    "    \n",
    "    if not all_features:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Extracted features from {len(all_features)} PDFs\")\n",
    "    return pd.DataFrame(all_features)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "fdbd2366-eec2-437c-82df-ef4f3b2d435f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:03.824053Z",
     "start_time": "2026-02-08T21:55:03.819183Z"
    }
   },
   "source": [
    "def create_features(df, pdf_features_df):\n",
    "    if not pdf_features_df.empty:\n",
    "        df = df.merge(pdf_features_df, on='patient_id', how='left')\n",
    "        pdf_cols = [c for c in pdf_features_df.columns if c != 'patient_id']\n",
    "        df[pdf_cols] = df[pdf_cols].fillna(0)\n",
    "    \n",
    "    df['cost_per_visit_ratio'] = df['prior_ed_cost_5y_usd'] / (df['prior_ed_visits_5y'] + 1)\n",
    "    df['log_prior_cost'] = np.log1p(df['prior_ed_cost_5y_usd'])\n",
    "    df['log_prior_visits'] = np.log1p(df['prior_ed_visits_5y'])\n",
    "    \n",
    "    if 'pdf_avg_cost_per_visit' in df.columns:\n",
    "        df['pdf_vs_table_cost_diff'] = df['pdf_total_cost'] - df['prior_ed_cost_5y_usd']\n",
    "        df['cost_consistency_ratio'] = df['pdf_total_cost'] / (df['prior_ed_cost_5y_usd'] + 1)\n",
    "        df['high_complexity_ratio'] = df['pdf_high_complexity_visits'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['lab_per_visit'] = df['pdf_num_lab_tests'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['imaging_per_visit'] = df['pdf_num_imaging'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['cost_variability'] = df['pdf_std_line_cost'] / (df['pdf_avg_line_cost'] + 1)\n",
    "        df['log_pdf_total_cost'] = np.log1p(df['pdf_total_cost'])\n",
    "    \n",
    "    if 'insurance' in df.columns:\n",
    "        df['insurance_private'] = (df['insurance'] == 'private').astype(int)\n",
    "        df['insurance_medicare'] = (df['insurance'] == 'medicare').astype(int)\n",
    "        df['insurance_medicaid'] = (df['insurance'] == 'medicaid').astype(int)\n",
    "    \n",
    "    if 'primary_chronic' in df.columns:\n",
    "        chronic_dummies = pd.get_dummies(df['primary_chronic'], prefix='chronic')\n",
    "        df = pd.concat([df, chronic_dummies], axis=1)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8f8e75ad-2818-401b-89a0-54f079933dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:08.554082Z",
     "start_time": "2026-02-08T21:55:03.824555Z"
    }
   },
   "source": [
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "print(\"Extracting PDF features...\")\n",
    "pdf_features = load_pdf_features(PDF_FOLDER)\n",
    "\n",
    "print(\"Engineering features...\")\n",
    "train_enhanced = create_features(train_df.copy(), pdf_features)\n",
    "test_enhanced = create_features(test_df.copy(), pdf_features)\n",
    "\n",
    "target_col = 'ed_cost_next3y_usd'\n",
    "exclude_cols = ['patient_id', target_col, 'primary_chronic', 'insurance']\n",
    "feature_cols = [c for c in train_enhanced.columns if c not in exclude_cols]\n",
    "\n",
    "train_enhanced[feature_cols] = train_enhanced[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "test_enhanced[feature_cols] = test_enhanced[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "X_train = train_enhanced[feature_cols]\n",
    "y_train = train_enhanced[target_col]\n",
    "X_test = test_enhanced[feature_cols]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features from 4000 PDFs\n",
      "Engineering features...\n",
      "Features: 29\n",
      "Training samples: 2000\n",
      "Test samples: 2000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "52a2cbb1-14f2-4772-b161-17d3959af3c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:09.976937Z",
     "start_time": "2026-02-08T21:55:08.567219Z"
    }
   },
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_val = StandardScaler()\n",
    "X_tr_scaled = scaler_val.fit_transform(X_tr)\n",
    "X_val_scaled = scaler_val.transform(X_val)\n",
    "\n",
    "val_model = xgb.XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.04,\n",
    "    max_depth=7,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "val_model.fit(X_tr_scaled, y_tr)\n",
    "val_pred = val_model.predict(X_val_scaled)\n",
    "val_mae = mean_absolute_error(y_val, val_pred)\n",
    "print(f\"Validation MAE: ${val_mae:.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: $449.22\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "bf1b4ae4-25ba-441a-a9b2-449d28cdea26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:11.395435Z",
     "start_time": "2026-02-08T21:55:09.994686Z"
    }
   },
   "source": [
    "print(\"\\nTraining final model on full data...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Best model: XGBoost\n",
    "final_model = xgb.XGBRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.04,\n",
    "    max_depth=7,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "predictions = final_model.predict(X_test_scaled)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'patient_id': test_df['patient_id'],\n",
    "    'ed_cost_next3y_usd': predictions\n",
    "})\n",
    "\n",
    "submission_file = \"healthcare_challenge2_predictions.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"\\nPredictions saved to {submission_file}\")\n",
    "print(f\"Predictions: {len(predictions)}\")\n",
    "print(f\"Mean: ${predictions.mean():.2f}, Median: ${np.median(predictions):.2f}\")\n",
    "print(f\"Range: ${predictions.min():.2f} to ${predictions.max():.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to healthcare_challenge2_predictions.csv\n",
      "Predictions: 2000\n",
      "Mean: $3918.46, Median: $3565.82\n",
      "Range: $854.03 to $10885.12\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "92401f53-7311-45b8-9f23-a814dc3d0daa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:11.445814Z",
     "start_time": "2026-02-08T21:55:11.413549Z"
    }
   },
   "source": [
    "# NOTE: The AgentDS benchmark leaderboard is no longer hosted.\n",
    "# Submission via BenchmarkClient is no longer available.\n",
    "# The score achieved was MAE = $480.33.\n",
    "\n",
    "print(\"Done.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "9a3251be-43f7-4569-8432-d1a24405aca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:55:30.917826Z",
     "start_time": "2026-02-08T21:55:11.453170Z"
    }
   },
   "source": [
    "# Healthcare Challenge 2 - Best Model Solution\n",
    "# Target: < $450 MAE\n",
    "\n",
    "# ============================================================\n",
    "# 2. PDF EXTRACTION\n",
    "# ============================================================\n",
    "def extract_pdf_features(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "        \n",
    "        features = {}\n",
    "        patient_match = re.search(r'Patient ID:\\s*(\\d+)', text)\n",
    "        if not patient_match:\n",
    "            return None\n",
    "        features['patient_id'] = int(patient_match.group(1))\n",
    "        \n",
    "        zip_match = re.search(r'ZIP3:\\s*(\\d+)', text)\n",
    "        features['zip3'] = int(zip_match.group(1)) if zip_match else 0\n",
    "        \n",
    "        ins_match = re.search(r'Insurance:\\s*(\\w+)', text)\n",
    "        features['insurance'] = ins_match.group(1).lower() if ins_match else 'unknown'\n",
    "        \n",
    "        total_match = re.search(r'TOTAL\\s+([\\d,]+\\.?\\d*)', text)\n",
    "        features['pdf_total_cost'] = float(total_match.group(1).replace(',', '')) if total_match else 0\n",
    "        \n",
    "        cpt_pattern = r'(\\d{5})\\s+(.+?)\\s+(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)'\n",
    "        cpt_matches = re.findall(cpt_pattern, text)\n",
    "        features['pdf_num_line_items'] = len(cpt_matches)\n",
    "        \n",
    "        if cpt_matches:\n",
    "            cpt_codes = [match[0] for match in cpt_matches]\n",
    "            ed_visit_codes = [c for c in cpt_codes if c.startswith('9928')]\n",
    "            features['pdf_num_ed_visits'] = len(ed_visit_codes)\n",
    "            features['pdf_high_complexity_visits'] = sum(1 for c in ed_visit_codes if c in ['99284', '99285'])\n",
    "            features['pdf_med_complexity_visits'] = sum(1 for c in ed_visit_codes if c == '99283')\n",
    "            features['pdf_num_lab_tests'] = sum(1 for c in cpt_codes if 80000 <= int(c) <= 89999)\n",
    "            features['pdf_num_imaging'] = sum(1 for c in cpt_codes if 70000 <= int(c) <= 79999)\n",
    "            \n",
    "            line_costs = [float(match[4]) for match in cpt_matches]\n",
    "            features['pdf_avg_line_cost'] = np.mean(line_costs)\n",
    "            features['pdf_max_line_cost'] = np.max(line_costs)\n",
    "            features['pdf_min_line_cost'] = np.min(line_costs)\n",
    "            features['pdf_std_line_cost'] = np.std(line_costs)\n",
    "            features['pdf_avg_cost_per_visit'] = features['pdf_total_cost'] / features['pdf_num_ed_visits'] if features['pdf_num_ed_visits'] > 0 else 0\n",
    "        else:\n",
    "            for key in ['pdf_num_ed_visits', 'pdf_high_complexity_visits', 'pdf_med_complexity_visits',\n",
    "                       'pdf_num_lab_tests', 'pdf_num_imaging', 'pdf_avg_line_cost', 'pdf_max_line_cost', \n",
    "                       'pdf_min_line_cost', 'pdf_std_line_cost', 'pdf_avg_cost_per_visit']:\n",
    "                features[key] = 0\n",
    "        \n",
    "        return features\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def load_pdf_features(pdf_folder):\n",
    "    pdf_folder = Path(pdf_folder)\n",
    "    pdf_files = list(pdf_folder.glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    all_features = []\n",
    "    for pdf_file in pdf_files:\n",
    "        features = extract_pdf_features(pdf_file)\n",
    "        if features:\n",
    "            all_features.append(features)\n",
    "    \n",
    "    if not all_features:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Extracted features from {len(all_features)} PDFs\")\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "# ============================================================\n",
    "# 3. ENHANCED FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "def create_features(df, pdf_features_df):\n",
    "    if not pdf_features_df.empty:\n",
    "        df = df.merge(pdf_features_df, on='patient_id', how='left')\n",
    "        pdf_cols = [c for c in pdf_features_df.columns if c != 'patient_id']\n",
    "        df[pdf_cols] = df[pdf_cols].fillna(0)\n",
    "    \n",
    "    # Basic ratios\n",
    "    df['cost_per_visit_ratio'] = df['prior_ed_cost_5y_usd'] / (df['prior_ed_visits_5y'] + 1)\n",
    "    \n",
    "    # Log transforms\n",
    "    df['log_prior_cost'] = np.log1p(df['prior_ed_cost_5y_usd'])\n",
    "    df['log_prior_visits'] = np.log1p(df['prior_ed_visits_5y'])\n",
    "    df['sqrt_prior_cost'] = np.sqrt(df['prior_ed_cost_5y_usd'])\n",
    "    \n",
    "    # Polynomial features\n",
    "    df['prior_cost_squared'] = df['prior_ed_cost_5y_usd'] ** 2\n",
    "    df['prior_visits_squared'] = df['prior_ed_visits_5y'] ** 2\n",
    "    df['cost_visit_interaction'] = df['prior_ed_cost_5y_usd'] * df['prior_ed_visits_5y']\n",
    "    \n",
    "    if 'pdf_avg_cost_per_visit' in df.columns:\n",
    "        # PDF features\n",
    "        df['pdf_vs_table_cost_diff'] = df['pdf_total_cost'] - df['prior_ed_cost_5y_usd']\n",
    "        df['pdf_vs_table_visits_diff'] = df['pdf_num_ed_visits'] - df['prior_ed_visits_5y']\n",
    "        df['cost_consistency_ratio'] = df['pdf_total_cost'] / (df['prior_ed_cost_5y_usd'] + 1)\n",
    "        df['high_complexity_ratio'] = df['pdf_high_complexity_visits'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['med_complexity_ratio'] = df['pdf_med_complexity_visits'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['lab_per_visit'] = df['pdf_num_lab_tests'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['imaging_per_visit'] = df['pdf_num_imaging'] / (df['pdf_num_ed_visits'] + 1)\n",
    "        df['cost_variability'] = df['pdf_std_line_cost'] / (df['pdf_avg_line_cost'] + 1)\n",
    "        \n",
    "        # Service intensity score\n",
    "        df['service_intensity'] = (\n",
    "            df['pdf_high_complexity_visits'] * 3 + \n",
    "            df['pdf_med_complexity_visits'] * 2 + \n",
    "            df['pdf_num_lab_tests'] * 1 + \n",
    "            df['pdf_num_imaging'] * 2\n",
    "        ) / (df['pdf_num_ed_visits'] + 1)\n",
    "        \n",
    "        df['log_pdf_total_cost'] = np.log1p(df['pdf_total_cost'])\n",
    "        df['log_pdf_avg_cost'] = np.log1p(df['pdf_avg_cost_per_visit'])\n",
    "        \n",
    "        # Cost range features\n",
    "        df['pdf_cost_range'] = df['pdf_max_line_cost'] - df['pdf_min_line_cost']\n",
    "        df['pdf_avg_per_table_cost'] = df['pdf_avg_cost_per_visit'] / (df['cost_per_visit_ratio'] + 1)\n",
    "    \n",
    "    if 'insurance' in df.columns:\n",
    "        df['insurance_private'] = (df['insurance'] == 'private').astype(int)\n",
    "        df['insurance_medicare'] = (df['insurance'] == 'medicare').astype(int)\n",
    "        df['insurance_medicaid'] = (df['insurance'] == 'medicaid').astype(int)\n",
    "    \n",
    "    if 'primary_chronic' in df.columns:\n",
    "        chronic_dummies = pd.get_dummies(df['primary_chronic'], prefix='chronic')\n",
    "        df = pd.concat([df, chronic_dummies], axis=1)\n",
    "    \n",
    "    if 'zip3' in df.columns:\n",
    "        df['zip3_normalized'] = df['zip3'] / 100.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 4. LOAD DATA\n",
    "# ============================================================\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "print(\"Extracting PDF features...\")\n",
    "pdf_features = load_pdf_features(PDF_FOLDER)\n",
    "\n",
    "print(\"Engineering features...\")\n",
    "train_enhanced = create_features(train_df.copy(), pdf_features)\n",
    "test_enhanced = create_features(test_df.copy(), pdf_features)\n",
    "\n",
    "target_col = 'ed_cost_next3y_usd'\n",
    "exclude_cols = ['patient_id', target_col, 'primary_chronic', 'insurance']\n",
    "feature_cols = [c for c in train_enhanced.columns if c not in exclude_cols]\n",
    "\n",
    "train_enhanced[feature_cols] = train_enhanced[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "test_enhanced[feature_cols] = test_enhanced[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "X_train = train_enhanced[feature_cols]\n",
    "y_train = train_enhanced[target_col]\n",
    "X_test = test_enhanced[feature_cols]\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. VALIDATION WITH ENSEMBLE\n",
    "# ============================================================\n",
    "print(\"\\nValidating ensemble...\")\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_val = StandardScaler()\n",
    "X_tr_scaled = scaler_val.fit_transform(X_tr)\n",
    "X_val_scaled = scaler_val.transform(X_val)\n",
    "\n",
    "# Train 5 models\n",
    "models_val = []\n",
    "\n",
    "# XGBoost\n",
    "xgb_val = xgb.XGBRegressor(n_estimators=700, learning_rate=0.03, max_depth=8, \n",
    "                           min_child_weight=2, subsample=0.8, colsample_bytree=0.8,\n",
    "                           gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=-1)\n",
    "xgb_val.fit(X_tr_scaled, y_tr)\n",
    "models_val.append(('XGB', xgb_val, X_val_scaled, 0.35))\n",
    "\n",
    "# GradientBoosting\n",
    "gb_val = GradientBoostingRegressor(n_estimators=500, learning_rate=0.03, max_depth=7,\n",
    "                                   subsample=0.8, min_samples_split=8, random_state=42)\n",
    "gb_val.fit(X_tr_scaled, y_tr)\n",
    "models_val.append(('GB', gb_val, X_val_scaled, 0.30))\n",
    "\n",
    "# RandomForest\n",
    "rf_val = RandomForestRegressor(n_estimators=500, max_depth=20, min_samples_split=6,\n",
    "                               min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
    "rf_val.fit(X_tr, y_tr)\n",
    "models_val.append(('RF', rf_val, X_val, 0.15))\n",
    "\n",
    "# ExtraTrees\n",
    "et_val = ExtraTreesRegressor(n_estimators=500, max_depth=20, min_samples_split=6,\n",
    "                             min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
    "et_val.fit(X_tr, y_tr)\n",
    "models_val.append(('ET', et_val, X_val, 0.15))\n",
    "\n",
    "# Ridge\n",
    "ridge_val = Ridge(alpha=5.0, random_state=42)\n",
    "ridge_val.fit(X_tr_scaled, y_tr)\n",
    "models_val.append(('Ridge', ridge_val, X_val_scaled, 0.05))\n",
    "\n",
    "# Ensemble validation predictions\n",
    "val_preds = []\n",
    "for name, model, X_v, weight in models_val:\n",
    "    pred = model.predict(X_v)\n",
    "    mae = mean_absolute_error(y_val, pred)\n",
    "    val_preds.append(pred * weight)\n",
    "    print(f\"{name} ({weight*100:.0f}%): MAE ${mae:.2f}\")\n",
    "\n",
    "ensemble_val_pred = np.sum(val_preds, axis=0)\n",
    "val_mae = mean_absolute_error(y_val, ensemble_val_pred)\n",
    "print(f\"\\nEnsemble Validation MAE: ${val_mae:.2f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. TRAIN FINAL ENSEMBLE ON FULL DATA\n",
    "# ============================================================\n",
    "print(\"\\nTraining final ensemble on full data...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "final_models = []\n",
    "\n",
    "# XGBoost\n",
    "xgb_final = xgb.XGBRegressor(n_estimators=700, learning_rate=0.03, max_depth=8,\n",
    "                             min_child_weight=2, subsample=0.8, colsample_bytree=0.8,\n",
    "                             gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=-1)\n",
    "xgb_final.fit(X_train_scaled, y_train)\n",
    "final_models.append((xgb_final, X_test_scaled, 0.35))\n",
    "\n",
    "# GradientBoosting\n",
    "gb_final = GradientBoostingRegressor(n_estimators=500, learning_rate=0.03, max_depth=7,\n",
    "                                     subsample=0.8, min_samples_split=8, random_state=42)\n",
    "gb_final.fit(X_train_scaled, y_train)\n",
    "final_models.append((gb_final, X_test_scaled, 0.30))\n",
    "\n",
    "# RandomForest\n",
    "rf_final = RandomForestRegressor(n_estimators=500, max_depth=20, min_samples_split=6,\n",
    "                                 min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
    "rf_final.fit(X_train, y_train)\n",
    "final_models.append((rf_final, X_test, 0.15))\n",
    "\n",
    "# ExtraTrees\n",
    "et_final = ExtraTreesRegressor(n_estimators=500, max_depth=20, min_samples_split=6,\n",
    "                                min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
    "et_final.fit(X_train, y_train)\n",
    "final_models.append((et_final, X_test, 0.15))\n",
    "\n",
    "# Ridge\n",
    "ridge_final = Ridge(alpha=5.0, random_state=42)\n",
    "ridge_final.fit(X_train_scaled, y_train)\n",
    "final_models.append((ridge_final, X_test_scaled, 0.05))\n",
    "\n",
    "# Ensemble predictions\n",
    "test_preds = []\n",
    "for model, X_te, weight in final_models:\n",
    "    pred = model.predict(X_te)\n",
    "    test_preds.append(pred * weight)\n",
    "\n",
    "predictions = np.sum(test_preds, axis=0)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'patient_id': test_df['patient_id'],\n",
    "    'ed_cost_next3y_usd': predictions\n",
    "})\n",
    "\n",
    "submission_file = \"healthcare_challenge2_ensemble.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"\\nPredictions saved to {submission_file}\")\n",
    "print(f\"Mean: ${predictions.mean():.2f}, Median: ${np.median(predictions):.2f}\")\n",
    "\n",
    "# NOTE: The AgentDS benchmark leaderboard is no longer hosted.\n",
    "# Submission via BenchmarkClient is no longer available.\n",
    "# The final score achieved was MAE = $475.03.\n",
    "\n",
    "print(\"\\nDone.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to healthcare_challenge2_ensemble.csv\n",
      "Mean: $3920.70, Median: $3579.17\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "1d8793d7-7c43-42b3-ba55-5670531fc65c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T21:56:11.800570Z",
     "start_time": "2026-02-08T21:55:30.940474Z"
    }
   },
   "source": "# ============================================================\n# Score Estimation via 5-Fold Cross-Validation\n# (Test labels are not available on HuggingFace, so we estimate\n#  the MAE score using cross-validation on training data)\n# ============================================================\n\nprint(\"ðŸ“Š Estimating MAE via 5-fold CV on training data...\")\nprint(\"   Evaluating: Simple XGBoost vs Weighted Ensemble\\n\")\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# --- Simple XGBoost CV ---\nsimple_xgb_maes = []\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train), 1):\n    X_tr_f, X_val_f = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n    y_tr_f, y_val_f = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    \n    sc = StandardScaler()\n    X_tr_sc = sc.fit_transform(X_tr_f)\n    X_val_sc = sc.transform(X_val_f)\n    \n    m = xgb.XGBRegressor(n_estimators=600, learning_rate=0.04, max_depth=7,\n                         min_child_weight=3, subsample=0.8, colsample_bytree=0.8,\n                         gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=-1)\n    m.fit(X_tr_sc, y_tr_f)\n    mae = mean_absolute_error(y_val_f, m.predict(X_val_sc))\n    simple_xgb_maes.append(mae)\n    print(f\"  Fold {fold} - Simple XGBoost MAE: ${mae:.2f}\")\n\nprint(f\"  >> Simple XGBoost Mean MAE: ${np.mean(simple_xgb_maes):.2f} (+/- ${np.std(simple_xgb_maes):.2f})\\n\")\n\n# --- Weighted Ensemble CV ---\nensemble_maes = []\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train), 1):\n    X_tr_f, X_val_f = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n    y_tr_f, y_val_f = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    \n    sc = StandardScaler()\n    X_tr_sc = sc.fit_transform(X_tr_f)\n    X_val_sc = sc.transform(X_val_f)\n    \n    # XGBoost (35%)\n    xgb_m = xgb.XGBRegressor(n_estimators=700, learning_rate=0.03, max_depth=8,\n                              min_child_weight=2, subsample=0.8, colsample_bytree=0.8,\n                              gamma=0.1, reg_alpha=0.1, reg_lambda=1.0, random_state=42, n_jobs=-1)\n    xgb_m.fit(X_tr_sc, y_tr_f)\n    \n    # GradientBoosting (30%)\n    gb_m = GradientBoostingRegressor(n_estimators=500, learning_rate=0.03, max_depth=7,\n                                     subsample=0.8, min_samples_split=8, random_state=42)\n    gb_m.fit(X_tr_sc, y_tr_f)\n    \n    # RandomForest (15%) - no scaling\n    rf_m = RandomForestRegressor(n_estimators=500, max_depth=20, min_samples_split=6,\n                                 min_samples_leaf=3, random_state=42, n_jobs=-1)\n    rf_m.fit(X_tr_f, y_tr_f)\n    \n    # ExtraTrees (15%) - no scaling\n    et_m = ExtraTreesRegressor(n_estimators=500, max_depth=20, min_samples_split=6,\n                               min_samples_leaf=3, random_state=42, n_jobs=-1)\n    et_m.fit(X_tr_f, y_tr_f)\n    \n    # Ridge (5%)\n    ridge_m = Ridge(alpha=5.0)\n    ridge_m.fit(X_tr_sc, y_tr_f)\n    \n    # Weighted ensemble prediction\n    ens_pred = (\n        0.35 * xgb_m.predict(X_val_sc) +\n        0.30 * gb_m.predict(X_val_sc) +\n        0.15 * rf_m.predict(X_val_f) +\n        0.15 * et_m.predict(X_val_f) +\n        0.05 * ridge_m.predict(X_val_sc)\n    )\n    \n    mae = mean_absolute_error(y_val_f, ens_pred)\n    ensemble_maes.append(mae)\n    print(f\"  Fold {fold} - Ensemble MAE: ${mae:.2f}\")\n\nprint(f\"  >> Ensemble Mean MAE: ${np.mean(ensemble_maes):.2f} (+/- ${np.std(ensemble_maes):.2f})\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"  Simple XGBoost  5-Fold CV MAE:  ${np.mean(simple_xgb_maes):.2f}\")\nprint(f\"  Weighted Ensemble 5-Fold CV MAE: ${np.mean(ensemble_maes):.2f}\")\nprint(f\"{'='*60}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5 - Ensemble MAE: $454.78\n",
      "  >> Ensemble Mean MAE: $458.51 (+/- $19.93)\n",
      "\n",
      "============================================================\n",
      "  Simple XGBoost  5-Fold CV MAE:  $467.22\n",
      "  Weighted Ensemble 5-Fold CV MAE: $458.51\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
